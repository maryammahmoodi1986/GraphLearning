---
title: "ترتیب‌دهی گره‌ها"
---
[قبلی](section2-3.qmd) 

---


انتخاب ویژگی (Feature Selection) یکی از مراحل کلیدی در فرآیند مدل‌سازی داده‌هاست. این فرآیند به شناسایی و انتخاب مهم‌ترین ویژگی‌ها (متغیرها یا فیچرها) از مجموعه داده‌ها می‌پردازد و ویژگی‌های غیرضروری یا کم اهمیت را حذف می‌کند.

هدف اصلی از این کار، کاهش پیچیدگی مدل، جلوگیری از بیش‌برازش (Overfitting)، بهبود عملکرد کلی مدل و کاهش زمان محاسباتی است. در داده‌های با حجم بالا یا ابعاد زیاد، وجود ویژگی‌های نامرتبط می‌تواند مدل را گیج کند و دقت پیش‌بینی را کاهش دهد. بنابراین، انتخاب ویژگی نه تنها به بهینه‌سازی مدل کمک می‌کند، بلکه تفسیر نتایج را نیز آسان‌تر می‌سازد.

## طبقه‌بندی روش‌های انتخاب ویژگی

روش‌های انتخاب ویژگی به طور کلی به سه دسته اصلی تقسیم می‌شوند:

1. **روش‌های فیلتر (Filter Methods)**
2. **روش‌های پوششی (Wrapper Methods)**
3. **روش‌های تعبیه‌شده (Embedded Methods)**

علاوه بر این، روش‌های ترکیبی (Hybrid Methods) نیز وجود دارند که مزایای روش‌های قبلی را ترکیب می‌کنند.

### ۱. روش‌های فیلتر (Filter Methods)

روش‌های فیلتر بر پایه ویژگی‌های آماری داده‌ها عمل می‌کنند و مستقل از مدل پیش‌بینی هستند. این روش‌ها پیش از آموزش مدل اجرا می‌شوند و بر اساس رابطه بین ویژگی‌ها و متغیر هدف (Target) تصمیم‌گیری می‌کنند.

**مزیت اصلی**: سرعت بالا و سادگی
**نقطه ضعف**: ممکن است وابستگی‌های پیچیده بین ویژگی‌ها را نادیده بگیرند

#### تکنیک‌های اصلی فیلتر:

**تحلیل همبستگی (Correlation Analysis)**:
- همبستگی بین هر ویژگی و متغیر هدف محاسبه می‌شود
- از ضریب پیرسون (برای داده‌های پیوسته) یا اسپیرمن (برای داده‌های ترتیبی) استفاده می‌شود
- ویژگی‌های با همبستگی پایین حذف می‌شوند

**آزمون‌های آماری**:
- **Chi-Square Test**: برای ویژگی‌های دسته‌ای
- **ANOVA**: برای ویژگی‌های پیوسته
- **Mutual Information**: برای محاسبه اطالعات مشترک بین ویژگی و هدف
- **Variance Threshold**: برای حذف ویژگی‌های با واریانس پایین

**Information Gain**:
- میزان اطلاعاتی که هر ویژگی به پیش‌بینی هدف اضافه می‌کند را ارزیابی می‌کند
- ویژگی‌های با gain بالا نگه داشته می‌شوند

### ۲. روش‌های پوششی (Wrapper Methods)

روش‌های wrapper از یک مدل پیش‌بینی برای ارزیابی ترکیب‌های مختلف ویژگی‌ها استفاده می‌کنند. این روش‌ها وابسته به مدل هستند و عملکرد مدل را برای انتخاب بهترین زیرمجموعه ویژگی‌ها اندازه‌گیری می‌کنند.

**مزیت اصلی**: دقت بالا
**نقطه ضعف**: از نظر محاسباتی سنگین هستند

#### تکنیک‌های اصلی پوششی:

**Forward Selection**:
- از هیچ ویژگی شروع می‌شود و ویژگی‌ها یک‌یک اضافه می‌شوند
- در هر گام، ویژگی‌ای اضافه می‌شود که بیشترین بهبود را ایجاد کند

**Backward Elimination**:
- با تمام ویژگی‌ها شروع می‌شود و ویژگی‌های کم اهمیت حذف می‌گردند
- حذف ویژگی‌ای که کمترین تأثیر منفی بر عملکرد دارد، ادامه می‌یابد

**Recursive Feature Elimination (RFE)**:
- از مدل‌هایی مانند SVM یا درخت تصمیم برای وزن‌دهی ویژگی‌ها استفاده می‌کند
- کم اهمیت‌ترین‌ها را تدریجی حذف می‌نماید

### ۳. روش‌های تعبیه‌شده (Embedded Methods)

روش‌های embedded انتخاب ویژگی را در فرآیند آموزش مدل ادغام می‌کنند. برخی مدل‌ها به طور ذاتی قابلیت وزن‌دهی یا حذف ویژگی‌ها را دارند، که این روش را کارآمدتر از wrapper ها می‌سازد.

#### تکنیک‌های اصلی تعبیه‌شده:

**Regularization-Based Methods**:
- از تکنیک‌هایی مانند L1 Regularization (LASSO) برای صفر کردن وزن ویژگی‌های غیرضروری استفاده می‌شود
- Elastic Net (ترکینی از L1 و L2) نیز کاربرد دارد

**Tree-Based Models**:
- مدل‌هایی مانند Random Forest یا XGBoost اهمیت ویژگی‌ها را گزارش می‌دهند
- ویژگی‌های کم اهمیت حذف می‌شوند

**Gradient Boosting Machines (GBM)**:
- مدل‌هایی مانند LightGBM و CatBoost وزن‌دهی ویژگی‌ها را در فرآیند boosting تعیین می‌کنند

## روش‌های پیشرفته برای ترتیب‌دهی گره‌ها

در زمینه شبکه‌های بیزی، ترتیب گره‌ها (Node Ordering) برای الگوریتم‌هایی مانند K2 حیاتی است. این الگوریتم‌ها ساختار DAG (Directed Acyclic Graph) را بر اساس ترتیب گره‌ها می‌سازند.

### ۱. الگوریتم SORDER

SORDER برای انتخاب ترتیب متوالی گره‌ها در گراف بدون جهت طراحی شده است. ابتدا ماتریس همبستگی محاسبه می‌شود و گراف با یال‌هایی بین گره‌های با همبستگی بالا (بالای آستانه θ، مانند چارک سوم) ساخته می‌گردد. سپس، گره‌ها بر اساس درجه (Degree) مرتب می‌شوند.

**ویژگی‌ها**: ساده و بر پایه ساختار گراف

### ۲. تحلیل عاملی (Factor Analysis)

این روش مدل عاملی را برای بیان متغیرها بر اساس عوامل مشترک و خاص استفاده می‌کند. مدل به صورت $X - μ = LF + ξ$ است، جایی که L ماتریس بارگذاری‌هاست. واریانس هر متغیر به Communality (اشتراک‌پذیری) و Specific Variance تقسیم می‌شود. گره‌ها بر اساس communality نزولی مرتب می‌شوند، زیرا نشان‌دهنده میزان اطلاعات استخراج شده است.

**کاربرد**: برای داده‌های ترتیبی از همبستگی اسپیرمن استفاده می‌کند

### ۳. آنتروپی (Entropy)

آنتروپی معیاری از عدم قطعیت است. گره‌ها از کمترین آنتروپی (اطلاعات بیشتر) به بیشترین مرتب می‌شوند. 

**مثال**: برای گره‌های {A,B,C,D}، ترتیب (A,B,D,C) بر اساس H(A) < H(B) < H(D) < H(C) است.

**ویژگی‌ها**: ساده و مستقل از مدل

### ۴. اطلاعات متقابل (Mutual Information - MI)

MI وابستگی بین گره‌ها را اندازه‌گیری می‌کند. گره شروع (معمولاً اولویت کمترین وابستگی) انتخاب می‌شود، سپس گره بعدی بر اساس بیشترین MI با قبلی.

**ویژگی‌ها**: وابستگی‌های غیرخطی را در نظر می‌گیرد و سریع است

### ۵. آنتروپی شرطی (Conditional Entropy)

آنتروپی شرطی عدم قطعیت یک گره با شرط گره دیگر را محاسبه می‌کند. گره شروع با کمترین آنتروپی انتخاب می‌شود، سپس گره بعدی با کمترین آنتروپی شرطی.

**کاربرد**: وابستگی‌های شرطی را برجسته می‌کند

### ۶. روش امتیازدهی BIC

BIC برای ارزیابی DAG بر اساس ترتیب محاسبه می‌شود. در هر گام، گره‌ای که بیشترین BIC را ایجاد کند اضافه می‌گردد.

**ویژگی‌ها**: بهینه‌سازی مستقیم بر اساس امتیاز مدل

### ۷. روش ITNO (Information-Theory Node Ordering)

ITNO از MI برای ساخت ماتریس وابستگی استفاده می‌کند. جفت گره‌هایی با MI قوی‌تر از سایرین علامت‌گذاری می‌شوند. گره شروع بر اساس بیشترین وابستگی انتخاب می‌شود، سپس ترتیب حریصانه ادامه می‌یابد.

**پیچیدگی**: O(n³) - وابستگی‌های کلیدی را برجسته می‌کند

### ۸. الگوریتم ژنتیک

این روش جمعیت ترتیب‌ها را تولید کرده و با تقاطع و جهش، بهترین را بر اساس BIC انتخاب می‌کند.

**کاربرد**: مناسب برای جستجوی بهینه در فضای بزرگ

## مقایسه جامع روش‌ها

| روش | زمان مناسب استفاده |
|-----|-------------------|
| Filter Methods | زمانی که نیاز به پیش‌پردازش سریع و مستقل از مدل دارید |
| Wrapper Methods | زمانی که دقت بالا اهمیت دارد و منابع محاسباتی کافی موجود است |
| Embedded Methods | زمانی که انتخاب ویژگی در فرآیند آموزش مدل ادغام می‌شود |

## مقایسه تخصصی: روش ITNO و MI ساده

### ۱. روش ITNO

این روش از MI برای محاسبه وابستگی استفاده می‌کند و ماتریس وابستگی (dep_matrix) می‌سازد تا جفت‌های قوی را شناسایی کند. گره شروع بر اساس بیشترین وابستگی انتخاب می‌شود و ترتیب حریصانه ادامه می‌یابد.

**مشخصات**:
- پیچیدگی: O(n³)
- وابستگی‌های کلیدی را برجسته می‌کند

### ۲. روش MI ساده

گره شروع دستی (اولویتی) انتخاب می‌شود و گره بعدی بر اساس بیشترین MI با قبلی.

**مشخصات**:
- پیچیدگی: O(n²)
- ساده‌تر است، اما ممکن است وابستگی‌های پیچیده را نادیده بگیرد

### تفاوت‌های کلیدی

| معیار | ITNO | MI ساده |
|-------|------|----------|
| **انتخاب نقطه شروع** | خودکار (بر اساس وابستگی) | دستی |
| **معیار انتخاب** | از dep_matrix فیلتر شده | مستقیم از MI |
| **پیچیدگی** | بالاتر | کمتر |
| **رویکرد** | ساختاریافته | مستقیم |
| **مزایا/معایب** | دقیق‌تر اما سنگین‌تر | سریع‌تر اما ممکن است غیربهینه |

## نتیجه‌گیری

انتخاب روش مناسب ترتیب‌دهی گره‌ها به عوامل مختلفی بستگی دارد:

- **اندازه داده**: برای داده‌های بزرگ، روش‌های سریع‌تر ترجیح داده می‌شوند
- **پیچیدگی وابستگی**: برای وابستگی‌های پیچیده، روش‌های پیشرفته‌تر مناسب‌تر هستند
- **منابع محاسباتی**: محدودیت‌های زمانی و حافظه تأثیرگذار هستند
- **دقت مورد نیاز**: کاربردهای حساس به دقت نیاز به روش‌های پیشرفته‌تر دارند

در عمل، ترکیب چندین روش یا استفاده از روش‌های تطبیقی که بر اساس ویژگی‌های داده انتخاب می‌شوند، بهترین نتایج را ارائه می‌دهند.

---

[قبلی](section2-3.qmd) 
